{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import predictor\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pro_data():\n",
    "    \n",
    "    # generate expariment data with the shape [10000,2,10] for training features;\n",
    "    # with the shape [10000,1] for training label;\n",
    "    # with the shape [1000,2,10] for test features\n",
    "    # with the shape [1000,1] for test label\n",
    "    \n",
    "    def __init__(self,data_dict):\n",
    "        \n",
    "        self.ts = data_dict['ts']\n",
    "        self.pred_n = data_dict['pred_n']\n",
    "        self.n_train = data_dict['n_train']\n",
    "        self.n_test = data_dict['n_test']\n",
    "        self.d_t = data_dict['d_t']\n",
    "        self.time_steps = data_dict['time_steps']\n",
    "        self.ratio = data_dict['ratio']\n",
    "        self.file_name = data_dict['file_name']\n",
    "        \n",
    "    def detrend(self,order):\n",
    "    \n",
    "        data = pd.read_csv(self.file_name)\n",
    "        x = df.index.values\n",
    "        y = df.iloc[:,-1].values\n",
    "        poly = np.polyfit(x, y, deg = order)\n",
    "\n",
    "        return poly\n",
    "\n",
    "    def addtrend(self,x_dex,poly):\n",
    "        # x_dex: np.array. index of predict value\n",
    "\n",
    "        y = np.ployval(poly,x_dex)\n",
    "        return y\n",
    "\n",
    "    def norm_op(self,data,op,norm_opt):\n",
    "    \n",
    "        '''This  funtion is to apply normalization for inputdata, and return new data\n",
    "        Input:\n",
    "                  data: input data with the shape [N,1] for label, and [time,features] for features\n",
    "                  norm_opt: normalize type option, here have 3 different type 'norm','max_min','max_abs' now\n",
    "                  op: scalar, 1 for labels and 0 for features\n",
    "\n",
    "        Output:\n",
    "                 new_data: new data after normalization have the same shape with input data\n",
    "                 if op == 1, also return k and b for recover use'''\n",
    "        \n",
    "        # for features data  \n",
    "        if op == 0:\n",
    "            if norm_opt == 'max_min':\n",
    "                new_data = (data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0)) \n",
    "            elif norm_opt == 'norm':\n",
    "                new_data = (data-data.mean(axis=0))/(np.std(data,axis=0))\n",
    "            elif norm_opt == 'max_abs':\n",
    "                new_data = data/np.max(np.abs(data),axis=0)\n",
    "\n",
    "            return new_data\n",
    "\n",
    "        elif op == 1: # for labels data\n",
    "\n",
    "            if norm_opt == 'norm':  # use norm distribution and so-called normalization\n",
    "                b, k = np.mean(data), np.std(data)\n",
    "            elif norm_opt == 'max_min': # makes data range in the interval [-1 1 ] with substrct the minimum value\n",
    "                b, k = np.min(data), (np.max(data)-np.min(data))\n",
    "            elif norm_opt == 'max_abs': # makes data range in the intreval [0 1] with divide the absolute maximum valiue\n",
    "                b, k = 0,np.max(np.abs(data))\n",
    "            \n",
    "            return (data - b)/k,k,b\n",
    "    \n",
    "    \n",
    "\n",
    "    def read_file(self):\n",
    "\n",
    "        '''function is to get input from csv file(file_name),and we demand input file with\n",
    "         features each colunme and the time steps each rank. then we re-form the time\n",
    "         -series with time step length 'time_steps'\n",
    "\n",
    "         input:\n",
    "                    file_name: name of input csv file\n",
    "                    tine_steps: time window's length\n",
    "                    ratio: ratio for train and test'''\n",
    "        # here if data is too big, we can consider use generator,and when we use tf,\n",
    "        # can use with tf.data.from_generator\n",
    "    \n",
    "        data = pd.read_csv(self.file_name) # column 1 to -2 features; column -1 for labels\n",
    "        \n",
    "        # if have date-information we use\n",
    "        #data = np.array(data.iloc[:,1:])\n",
    "        \n",
    "        # if have no data infornation in the file, we use:\n",
    "        data = np.array(data)\n",
    "        \n",
    "        data_fea = self.norm_op(data[:,:-1],0,'max_abs')        #  input feature data with normlization\n",
    "        data_lab,k,b = self.norm_op(data[:,-1],1,'max_abs')   # inout label data witg normlization\n",
    "        \n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(data_fea.shape[0]- self.time_steps-self.pred_n+1):\n",
    "            features.append(data_fea[i:i+ self.time_steps,:])\n",
    "            labels.append(data_lab[i+ self.time_steps:i+self.time_steps+self.pred_n]) \n",
    "\n",
    "        features= np.array(features,dtype=np.float32).transpose((0,2,1))\n",
    "        labels = np.array(labels,dtype=np.float32)\n",
    "        labels = labels.reshape(labels.shape[0],self.pred_n)\n",
    "        \n",
    "        dex = round(data.shape[0]* self.ratio)\n",
    "\n",
    "        # return train_X train_y test_X test_y k b\n",
    "        return features[0:dex,:,:], labels[0:dex,:], features[dex:,:,:], labels[dex:,:],k,b\n",
    "    \n",
    "    \n",
    "    def de_norm(self,data):\n",
    "        '''\n",
    "        de-normaliz of data, for prediction use\n",
    "        Input:\n",
    "            data: data will be recover'\n",
    "            para: [2,1] list for k and b\n",
    "        '''\n",
    "        return data*self.k+self.b\n",
    "    \n",
    "    def exp_data(self,seq):\n",
    "\n",
    "        '''this function is to generate features and labels of training and test input data, \n",
    "            and only for experimental use. Here the input is index of time series, and we\n",
    "            construct the features and labels with following formula:\n",
    "\n",
    "            features = [cos(seq),sin(seq)]\n",
    "            labels = [sin(seq)*cos(seq)]\n",
    "\n",
    "            with shape [sample_number,features_number,time_steps] for features and\n",
    "            [sample_number] for labels\n",
    "\n",
    "            And finally return features and labels'''\n",
    "\n",
    "\n",
    "        features, labels = [], []\n",
    "        for i in range(len(seq)-self.ts-self.pred_n+1):\n",
    "            features.append([np.cos(seq[i: i + self.ts]),np.sin(seq[i: i + self.ts])])\n",
    "            labels.append([np.cos(seq[i+self.ts:i+self.ts+self.pred_n])*np.sin(seq[i+self.ts:i+self.ts+self.pred_n])])\n",
    "\n",
    "        return np.array(features, dtype=np.float32), np.array(labels, dtype=np.float32) \n",
    "    \n",
    "\n",
    "    def new_data(self):\n",
    "        \n",
    "        start = (self.n_train + self.ts) * self.d_t            # start value of input series\n",
    "        end = start + (self.n_test + self.ts) * self.d_t    # end value of input series\n",
    "        train_X, train_y = self.exp_data(np.linspace(0, start, self.n_train + self.ts, dtype=np.float32))\n",
    "        test_X, test_y = self.exp_data(np.linspace(start, end, self.n_test + self.ts, dtype=np.float32))\n",
    "        b,k = 0,1\n",
    "        \n",
    "        test_y = test_y.reshape(test_y.shape[0],test_y.shape[2])\n",
    "        train_y = train_y.reshape(train_y.shape[0],train_y.shape[2])\n",
    "        return train_X,train_y,test_X,test_y,k,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_model():\n",
    "    \n",
    "    def __init__(self,para):\n",
    "        \n",
    "        self.hidden_size = para['hidden_size']\n",
    "        self.num_layers = para['num_layers']\n",
    "        self.batch_size = para['batch_size']\n",
    "        self.epoch = para['epoch']\n",
    "        self.learning_rate = para['learning_rate']\n",
    "        self.shuffle_size = para['shuffle_size']\n",
    "        self.optimize = para['optimize']\n",
    "        self.train_step = para['train_step']\n",
    "        self.save_path = para['save_path']\n",
    "        self.pred_n = para['pred_n']\n",
    "        \n",
    "    def input_data(self,features,labels,mode):\n",
    "    \n",
    "    # data.dataset is an important senior API of tensoeflow for construct deeplearning\n",
    "    # algorithm. this API mainly use function such as from_tensor_slices, shuffle,map,\n",
    "    # repeat,batch and generator make_one_shot_iterator().get_next().\n",
    "    #\n",
    "    # here mainly have 3 ways to input numerical data into dataset:\n",
    "    # 1: from data(features and labels)\n",
    "    # 2: from file(sucn as with the format of tfrecord)\n",
    "    # 3: from tensor(tensor of features and tensor of labels)\n",
    "    #\n",
    "    # here we use .from_tensor_slices for input format, this way is suitable for data size \n",
    "    # not very big size, in this case can get data from generator.\n",
    "    # reference:\n",
    "    #                   https://tensorflow.google.cn/guide/performance/datasets\n",
    "    #                   https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "    \n",
    "        ds = tf.data.Dataset.from_tensor_slices({\"feature\": features, \"label\": labels}) # not need shuffle when eval\n",
    "        if mode == 'train':\n",
    "            ds = ds.shuffle(self.shuffle_size).repeat(self.epoch).batch(self.batch_size)\n",
    "        elif mode == 'eval' or mode == 'predict':\n",
    "            ds = ds.repeat(1).batch(self.batch_size)\n",
    "\n",
    "            \n",
    "        #x, y = ds.make_one_shot_iterator().get_next() # tensor of features and labels\n",
    "\n",
    "        rst = ds.make_one_shot_iterator().get_next() # tensor of features and labels\n",
    "        return rst['feature'], rst['label']\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------------------\n",
    "    def model_fn(self,  features,  labels,  mode):\n",
    "    \n",
    "    # construct the RNN net with num_layers layers and num_hidden units for each layer\n",
    "    # and return tf.estimator.EstimatorSpec for using tf.estimator\n",
    "    \n",
    "    # one thing need to remember is will we need multipl-processing?\n",
    "    \n",
    "        if isinstance(features, dict):  # For serving\n",
    "            features = features['feature']\n",
    "        \n",
    "        with tf.name_scope(\"RNN\"):\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(self.hidden_size) \n",
    "                                            for _ in range(self.num_layers)])  \n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, features, dtype=tf.float32)\n",
    "            output = outputs[:, -1, :]\n",
    "            \n",
    "            # addtion a full connection layer at last \n",
    "            predictions = tf.contrib.layers.fully_connected(output, self.pred_n, activation_fn=None)\n",
    "       \n",
    "        # for Predict use\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            \n",
    "            predict_output = {'values': predictions}\n",
    "            export_outputs = {'predictions': tf.estimator.export.PredictOutput(predict_output)}\n",
    "        \n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode = mode,\n",
    "                predictions = predict_output,\n",
    "                export_outputs=export_outputs)\n",
    "\n",
    " \n",
    "        # now mse, and can set this part optional\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
    "        \n",
    "        with tf.name_scope(\"Train\"):\n",
    "            train_op = tf.contrib.layers.optimize_loss(loss, tf.train.get_global_step(),\n",
    "                                                   optimizer=self.optimize, \n",
    "                                                   learning_rate=self.learning_rate)\n",
    "            metrics = {\"mae\": tf.metrics.mean_absolute_error(labels, predictions)} \n",
    "            tf.summary.scalar('Loss', loss)\n",
    "            \n",
    "        if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                loss=loss,\n",
    "                                                train_op=train_op)\n",
    "        \n",
    "    #----------------------------------------------------------------------------------\n",
    "    def create_estimator(self):\n",
    "        \n",
    "        training_config = tf.estimator.RunConfig(model_dir=self.save_path,tf_random_seed=1234)\n",
    "        estimator = tf.estimator.Estimator(model_fn=self.rnn_model, config=training_config)\n",
    "        return estimator\n",
    "    \n",
    "    \n",
    "    def serving_input_receiver_fn(self,xshape):\n",
    "        \n",
    "        number = tf.placeholder(dtype=tf.float32, shape=[None,xshape[0],xshape[1]], name='serve_input')\n",
    "        receiver_tensors = {'serve_input': number}\n",
    "        #features = tf.convert_to_tensor(np.zeros((1,xshape[0],xshape[1])),dtype=tf.float32)\n",
    "        features =  number\n",
    "        return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "        \n",
    "    def pred_model(self,test_X,test_y,md,k,b):\n",
    "        \n",
    "        '''test_X: features of  test/predict data\n",
    "        test_y: labels of test/predict data, for re_norm and for predict use\n",
    "        md: estimator\n",
    "        k: slope\n",
    "        b: interpret'''\n",
    "        \n",
    "        \n",
    "        results = md.predict(lambda:self.input_data(test_X,test_y,'eval'))\n",
    "        rst = [result[\"pred\"] for result in results]\n",
    "        rst = np.array(rst)\n",
    "\n",
    "        test_denorm = test_y*k+b\n",
    "        rst_denorm = rst*k+b\n",
    "        \n",
    "        return rst,rst_denorm,test_denorm\n",
    "\n",
    "    def plot_rst(self,test_y,rst):\n",
    "        \n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "        plt.plot(test_y, label=\"Actual Values\", color='red')\n",
    "        plt.plot(rst, label=\"Predicted Values\", color='green', )\n",
    "\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_rst2(self,test_y,rst):\n",
    "        \n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "        plt.plot(test_y[0], label=\"Actual Values\", color='red')\n",
    "        plt.plot(rst[0], label=\"Predicted Values\", color='green', )\n",
    "\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters dictionary for Class model\n",
    "# we can geuss the meanning of all parameters from the name of them\n",
    "\n",
    "para_dict = {'hidden_size':30,\n",
    "             'num_layers':1,\n",
    "             'batch_size':32,\n",
    "            'epoch':5,\n",
    "            'learning_rate':0.06,\n",
    "            'shuffle_size':10000,\n",
    "            'optimize':'Adam',\n",
    "            'train_step':3000,\n",
    "            'save_path':'./model',\n",
    "            'pred_n':1,\n",
    "            'save_model':'./serve_model'}\n",
    "\n",
    "data_dict = {'ts':200,\n",
    "             'pred_n':para_dict['pred_n'],\n",
    "            'n_train':10000,\n",
    "            'n_test':1000,\n",
    "            'd_t':0.1,\n",
    "            'time_steps':30,\n",
    "            'ratio':0.8,\n",
    "            'file_name':'/Users/kappa0517/Code/RNN_predict/data0416.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,test_X,test_y,k,b = pro_data(data_dict).new_data()\n",
    "s_p = (train_X.shape[1],train_X.shape[2])\n",
    "\n",
    "Path('model').mkdir(exist_ok=True)\n",
    "estimator = tf.estimator.Estimator(get_model(para_dict).model_fn, 'model')\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=lambda :get_model(para_dict).input_data(train_X,train_y,'train'))\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda :get_model(para_dict).input_data(test_X,test_y,'eval'),\n",
    "    exporters=[tf.estimator.LatestExporter(\n",
    "        name=\"eval\", \n",
    "        serving_input_receiver_fn=lambda :get_model(para_dict).serving_input_receiver_fn(s_p),\n",
    "        exports_to_keep=1,\n",
    "        as_text=True)],\n",
    "        steps=None)\n",
    "\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec, \n",
    "    eval_spec=eval_spec)\n",
    "\n",
    "estimator.export_saved_model(para_dict['save_model'], \n",
    "                             lambda :get_model(para_dict).serving_input_receiver_fn(s_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  预测\n",
    "Path(para_dict['save_path']).mkdir(exist_ok=True)\n",
    "estimator = tf.estimator.Estimator(get_model(para_dict).model_fn, para_dict['save_path'])\n",
    "results =estimator.predict(input_fn=lambda :get_model(para_dict).input_data(test_X,test_y,'predict'))\n",
    "rst = [result[\"values\"] for result in results]\n",
    "s = np.array(rst)\n",
    "# 服务\n",
    "#-----------------  提供服务\n",
    "#-----------------  path为文件夹时，Path.iterdir()产生path文件夹下的所有文件、文件夹路径的迭代器\n",
    "subdirs = [x for x in Path(para_dict['save_model']).iterdir() if x.is_dir() and 'temp' not in str(x)]\n",
    "latest = str(sorted(subdirs)[-1])\n",
    "predict_fn = predictor.from_saved_model(latest)\n",
    "\n",
    "a = test_X\n",
    "#a = np.random.random((30,s_p[0],s_p[1]))\n",
    "rst = []\n",
    "tic = time.time()\n",
    "for i in range(a.shape[0]):\n",
    "    \n",
    "    pred = predict_fn({'serve_input': [a[i,:,:]]})['values']\n",
    "    rst.append(pred[0])\n",
    "    \n",
    "rst = np.array(rst)\n",
    "rst2 = rst*k+b\n",
    "toc = time.time()\n",
    "\n",
    "print(toc-tic)\n",
    "#print(rst)\n",
    "print(s-rst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
